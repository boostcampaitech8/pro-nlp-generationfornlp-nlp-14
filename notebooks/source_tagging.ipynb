{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 출처 및 주제 태깅\n",
        "\n",
        "KMMLU, MMMLU, KLUE-MRC에서 가져온 학습 데이터를 식별해 `from`/`subject` 태그를 붙이는 정리된 노트북입니다.\\\n",
        "실행 전후로 `source_restoration.ipynb`를 건드릴 필요 없도록 전체 파이프라인을 한 번에 정돈했습니다.\n",
        "\n",
        "*작업을 수행하고나면, problems가 평탄화 된 상태로 저장됩니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 노트북 사용 가이드\n",
        "\n",
        "- `../data/train.csv`를 로드해 `problems` 컬럼을 펼친 뒤 라벨링합니다.\n",
        "- KMMLU(한국사)는 문단 기준 유사도 + 포함 검색을 활용합니다.\n",
        "- MMMLU는 화이트리스트된 과목만 사용해 앵커 기반 주제 매핑을 수행합니다.\n",
        "- KLUE-MRC는 허용된 뉴스 카테고리만 대상으로 앵커 매핑을 수행합니다.\n",
        "- 모든 단계가 끝나면 `../data/train_source_labeled.csv`를 생성합니다.(정상 실행 시점에만 저장).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import ast\n",
        "import re\n",
        "\n",
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "\n",
        "# 경로 설정\n",
        "DATA_DIR = Path(\"../data\")\n",
        "TRAIN_PATH = DATA_DIR / \"train.csv\"\n",
        "OUTPUT_PATH = DATA_DIR / \"train_source_labeled.csv\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 유틸 함수 모음\n",
        "- 텍스트 정규화: 공백/특수문자 제거 후 비교 안정화\n",
        "- 3-앵커 추출: 앞/중간/뒤 고정 길이 토막으로 매칭 강건성 확보\n",
        "- 키 후보 생성 및 빈도 기반 주제 매핑: 오탐을 줄이기 위해 최소 길이·화이트리스트 적용\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "_ws = re.compile(r\"\\s+\")\n",
        "_keep = re.compile(r\"[^0-9A-Za-z가-힣\\s]\")  # 허용: 한글/영문/숫자/공백\n",
        "\n",
        "def norm_text(x, remove_all_space: bool = True) -> str:\n",
        "    '''Whitespace/특수문자를 정리해 비교용 텍스트를 만든다.'''\n",
        "    if x is None or (isinstance(x, float) and pd.isna(x)):\n",
        "        return \"\"\n",
        "    x = str(x)\n",
        "    x = _ws.sub(\" \", x)\n",
        "    x = _keep.sub(\" \", x)\n",
        "    x = _ws.sub(\" \", x).strip()\n",
        "    if remove_all_space:\n",
        "        x = re.sub(r\"\\s+\", \"\", x)\n",
        "    return x\n",
        "\n",
        "def split_question_paragraph(text: str):\n",
        "    '''첫 번째 물음표 이전을 question, 이후를 paragraph로 분리.'''\n",
        "    if not isinstance(text, str):\n",
        "        return \"\", \"\"\n",
        "    text = text.strip()\n",
        "    qmark = text.find(\"?\")\n",
        "    if qmark == -1:\n",
        "        return text, \"\"\n",
        "    return text[: qmark + 1].strip(), text[qmark + 1 :].strip()\n",
        "\n",
        "def build_anchor_views(series: pd.Series, anchor_len: int):\n",
        "    '''문장을 앞/중간/끝 고정 길이 토막으로 나눠 앵커 세트를 만든다.'''\n",
        "    series = series.astype(\"string\").fillna(\"\")\n",
        "    head = series.str.slice(0, anchor_len)\n",
        "    tail = series.str.slice(-anchor_len, None)\n",
        "    lens = series.str.len().to_numpy()\n",
        "    mid_starts = (lens // 2) - (anchor_len // 2)\n",
        "    mid = pd.Series(\n",
        "        [txt[max(0, st): max(0, st) + anchor_len] for txt, st in zip(series.tolist(), mid_starts)],\n",
        "        index=series.index,\n",
        "        dtype=\"string\",\n",
        "    )\n",
        "    return head, mid, tail\n",
        "\n",
        "def make_keys(s: pd.Series, min_len: int = 2) -> pd.Series:\n",
        "    s = s.astype(\"string\").fillna(\"\")\n",
        "    s = s[s.str.len() >= min_len].drop_duplicates()\n",
        "    return s.sort_values(key=lambda x: x.str.len(), ascending=False)\n",
        "\n",
        "def collect_found_keys(corpus: pd.Series, keys: pd.Series, chunk_size: int = 3000) -> set:\n",
        "    '''코퍼스에서 keys 중 등장하는 것만 모아 검색 범위를 줄인다.'''\n",
        "    corpus = corpus.astype(\"string\").fillna(\"\")\n",
        "    keys_list = keys.astype(\"string\").fillna(\"\").tolist()\n",
        "    found = set()\n",
        "    for i in range(0, len(keys_list), chunk_size):\n",
        "        sub = keys_list[i:i + chunk_size]\n",
        "        pat = re.compile(\"|\".join(map(re.escape, sub)))\n",
        "        for lst in corpus.str.findall(pat):\n",
        "            if lst:\n",
        "                found.update(lst)\n",
        "    return found\n",
        "\n",
        "def build_top_subject_map(df: pd.DataFrame, keys: pd.Series, q_col: str, subj_col: str, chunk_size: int = 2000) -> dict:\n",
        "    '''질문/지문에서 발견된 key별 최빈 주제를 계산한다.'''\n",
        "    mm = df[[q_col, subj_col]].copy()\n",
        "    mm[q_col] = mm[q_col].astype(\"string\").fillna(\"\")\n",
        "    mm[subj_col] = mm[subj_col].astype(\"string\").fillna(\"\")\n",
        "    counts = {}\n",
        "    keys_list = keys.astype(\"string\").fillna(\"\").tolist()\n",
        "    for i in range(0, len(keys_list), chunk_size):\n",
        "        sub = keys_list[i:i + chunk_size]\n",
        "        pat = re.compile(\"|\".join(map(re.escape, sub)))\n",
        "        matches = mm[q_col].str.findall(pat)\n",
        "        for subj, lst in zip(mm[subj_col].tolist(), matches.tolist()):\n",
        "            if lst:\n",
        "                for k in lst:\n",
        "                    counts.setdefault(k, {}).setdefault(subj, 0)\n",
        "                    counts[k][subj] += 1\n",
        "    return {k: max(subj_count.items(), key=lambda x: x[1])[0] for k, subj_count in counts.items()}\n",
        "\n",
        "def propagate_labels(df: pd.DataFrame, norm_col: str = \"para_norm\") -> pd.DataFrame:\n",
        "    '''동일한 정규화 텍스트가 이미 라벨링된 경우 나머지 행에도 전파.'''\n",
        "    labeled = (\n",
        "        df[df[\"from\"].notna()][[norm_col, \"from\", \"subject\"]]\n",
        "        .drop_duplicates(subset=[norm_col])\n",
        "        .set_index(norm_col)\n",
        "    )\n",
        "    for col in [\"from\", \"subject\"]:\n",
        "        missing = df[col].isna()\n",
        "        df.loc[missing, col] = df.loc[missing, norm_col].map(labeled[col])\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 학습 데이터 로드 및 기본 전처리\n",
        "- `problems` 컬럼을 안전하게 dict로 변환해 question/answer/choices로 분리합니다.\n",
        "- 비교용 `para_norm`을 한 번 만들어 이후 단계에서 재사용합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train = pd.read_csv(TRAIN_PATH)\n",
        "parsed = train[\"problems\"].map(ast.literal_eval)\n",
        "train[\"question\"] = parsed.map(lambda x: x[\"question\"])\n",
        "train[\"answer\"] = parsed.map(lambda x: x[\"answer\"])\n",
        "train[\"choices\"] = parsed.map(lambda x: x[\"choices\"])\n",
        "train[\"paragraph\"] = train[\"paragraph\"].astype(str)\n",
        "\n",
        "train_origin = train.copy()\n",
        "train_origin[\"para_norm\"] = train_origin[\"paragraph\"].map(lambda x: norm_text(x, remove_all_space=True))\n",
        "\n",
        "train_labeled = train_origin.copy()\n",
        "train_labeled[\"from\"] = pd.NA\n",
        "train_labeled[\"subject\"] = pd.NA\n",
        "\n",
        "print(\"train missing paragraphs:\", train_origin[\"paragraph\"].eq(\"\").sum())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## KMMLU (한국사) 태깅\n",
        "1) KMMLU 전체(split 병합)를 question/paragraph로 분리 후 정규화합니다.\n",
        "2) train 문단의 앞쪽 앵커/짧은 문장 키가 KMMLU 지문에 등장하는지 검색합니다.\n",
        "3) KMMLU 질문/지문이 train 문단을 포함하는 경우까지 보완해 누락을 최소화합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1) 데이터셋 적재 및 전처리\n",
        "ds_kmmlu = load_dataset(\"HAERAE-HUB/KMMLU\", \"Korean-History\")\n",
        "kmmlu_raw = pd.concat([ds_kmmlu[s].to_pandas() for s in [\"train\", \"dev\", \"test\"]])\n",
        "\n",
        "kmmlu_proc = kmmlu_raw.copy()\n",
        "kmmlu_proc[[\"question\", \"paragraph\"]] = kmmlu_proc[\"question\"].apply(lambda x: pd.Series(split_question_paragraph(x)))\n",
        "kmmlu_proc[\"choices\"] = kmmlu_proc[[\"A\", \"B\", \"C\", \"D\"]].values.tolist()\n",
        "kmmlu_proc = kmmlu_proc[[\"question\", \"paragraph\", \"choices\", \"answer\"]].reset_index(drop=True)\n",
        "kmmlu_proc[\"para_norm\"] = kmmlu_proc[\"paragraph\"].map(lambda x: norm_text(x, remove_all_space=True))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2) 앵커 기반 매칭 (train -> KMMLU)\n",
        "remain = train_labeled[\"from\"].isna()\n",
        "train_key = train_origin[\"para_norm\"].astype(\"string\").fillna(\"\")\n",
        "\n",
        "ANCHOR_LEN = 50\n",
        "train_anchor = train_key.str.slice(0, ANCHOR_LEN)\n",
        "LONG_MIN, SHORT_MIN, SHORT_MAX = 30, 8, 29\n",
        "\n",
        "long_keys = make_keys(train_anchor[remain & (train_key.str.len() >= LONG_MIN)], min_len=LONG_MIN)\n",
        "short_keys = make_keys(train_key[remain & train_key.str.len().between(SHORT_MIN, SHORT_MAX)], min_len=SHORT_MIN)\n",
        "\n",
        "found_long = collect_found_keys(kmmlu_proc[\"para_norm\"], long_keys, chunk_size=1000)\n",
        "found_short = collect_found_keys(kmmlu_proc[\"para_norm\"], short_keys, chunk_size=1000)\n",
        "\n",
        "m = remain & (train_anchor.isin(found_long) | train_key.isin(found_short))\n",
        "train_labeled.loc[m, \"from\"] = \"KMMLU\"\n",
        "train_labeled.loc[m, \"subject\"] = \"korean_history\"\n",
        "train_labeled = propagate_labels(train_labeled)\n",
        "\n",
        "print(\"KMMLU(anchor) newly labeled:\", int(m.sum()))\n",
        "print(\"Total labeled so far:\", int(train_labeled[\"from\"].notna().sum()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3) 역방향 포함 검색 (KMMLU 질문/지문 -> train 문단)\n",
        "remain_idx = train_labeled.index[train_labeled[\"from\"].isna()]\n",
        "train_text = train_origin.loc[remain_idx, \"para_norm\"].astype(\"string\").fillna(\"\")\n",
        "\n",
        "MIN_KEY_LEN = 20\n",
        "km_para_keys = kmmlu_proc[\"para_norm\"].astype(\"string\").fillna(\"\").drop_duplicates()\n",
        "km_para_keys = km_para_keys[km_para_keys.str.len() >= MIN_KEY_LEN].sort_values(key=lambda s: s.str.len(), ascending=False)\n",
        "\n",
        "km_q_norm = kmmlu_proc[\"question\"].map(lambda x: norm_text(x, remove_all_space=True)).astype(\"string\").fillna(\"\")\n",
        "km_q_keys = km_q_norm.drop_duplicates()\n",
        "km_q_keys = km_q_keys[km_q_keys.str.len() >= MIN_KEY_LEN].sort_values(key=lambda s: s.str.len(), ascending=False)\n",
        "\n",
        "matched = {}\n",
        "for idx, txt in zip(remain_idx.tolist(), train_text.tolist()):\n",
        "    for k in km_para_keys:\n",
        "        if k in txt:\n",
        "            matched[idx] = k\n",
        "            break\n",
        "    if idx in matched:\n",
        "        continue\n",
        "    for k in km_q_keys:\n",
        "        if k in txt:\n",
        "            matched[idx] = k\n",
        "            break\n",
        "\n",
        "km_mask = train_labeled.index.isin(matched.keys())\n",
        "train_labeled.loc[km_mask, \"from\"] = \"KMMLU\"\n",
        "train_labeled.loc[km_mask, \"subject\"] = \"korean_history\"\n",
        "train_labeled = propagate_labels(train_labeled)\n",
        "\n",
        "print(\"KMMLU(containment) newly labeled:\", int(km_mask.sum()))\n",
        "print(\"Total labeled so far:\", int(train_labeled[\"from\"].notna().sum()))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## MMMLU 태깅 (화이트리스트 과목)\n",
        "- KO_KR split 전체를 합쳐 사용합니다.\n",
        "- 화이트리스트에 포함된 과목만 대상으로 앵커 기반 주제 매핑을 수행합니다.\n",
        "- 질문 텍스트 기반 보정 단계까지 수행해 누락된 히스토리를 채웁니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ds_mmmlu = load_dataset(\"openai/MMMLU\", \"KO_KR\")\n",
        "mmmlu_raw = pd.concat([ds_mmmlu[s].to_pandas() for s in ds_mmmlu.keys()], ignore_index=True)\n",
        "\n",
        "mmmlu_proc = mmmlu_raw.copy()\n",
        "mmmlu_proc[\"q_norm\"] = mmmlu_proc[\"Question\"].map(lambda x: norm_text(x, remove_all_space=True)).astype(\"string\").fillna(\"\")\n",
        "mmmlu_proc[\"Subject\"] = mmmlu_proc[\"Subject\"].astype(\"string\").fillna(\"\")\n",
        "\n",
        "WHITE_LIST = {\n",
        "    \"high_school_european_history\",\n",
        "    \"high_school_us_history\",\n",
        "    \"high_school_world_history\",\n",
        "    \"high_school_macroeconomics\",\n",
        "    \"high_school_microeconomics\",\n",
        "    \"high_school_government_and_politics\",\n",
        "    \"high_school_geography\",\n",
        "    \"high_school_psychology\",\n",
        "}\n",
        "mmmlu_proc = mmmlu_proc[mmmlu_proc[\"Subject\"].isin(WHITE_LIST)].copy()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1) 문단 기반 앵커 매핑\n",
        "remain = train_labeled[\"from\"].isna()\n",
        "s = train_origin[\"para_norm\"].astype(\"string\").fillna(\"\")\n",
        "a_head, a_mid, a_tail = build_anchor_views(s, anchor_len=40)\n",
        "\n",
        "LONG_MIN, SHORT_MIN, SHORT_MAX = 20, 8, 29\n",
        "long_keys = make_keys(pd.concat([\n",
        "    a_head[remain & (s.str.len() >= LONG_MIN)],\n",
        "    a_mid[remain & (s.str.len() >= LONG_MIN)],\n",
        "    a_tail[remain & (s.str.len() >= LONG_MIN)],\n",
        "], ignore_index=True).drop_duplicates(), min_len=LONG_MIN)\n",
        "short_keys = make_keys(s[remain & s.str.len().between(SHORT_MIN, SHORT_MAX)].drop_duplicates(), min_len=SHORT_MIN)\n",
        "\n",
        "top_long = build_top_subject_map(mmmlu_proc, long_keys, q_col=\"q_norm\", subj_col=\"Subject\", chunk_size=1000)\n",
        "top_short = build_top_subject_map(mmmlu_proc, short_keys, q_col=\"q_norm\", subj_col=\"Subject\", chunk_size=500)\n",
        "\n",
        "m_long = remain & (a_head.isin(top_long) | a_mid.isin(top_long) | a_tail.isin(top_long))\n",
        "m_short = remain & s.isin(top_short)\n",
        "m = m_long | m_short\n",
        "\n",
        "subj_long = a_head.map(top_long).fillna(a_mid.map(top_long)).fillna(a_tail.map(top_long))\n",
        "subj_short = s.map(top_short)\n",
        "\n",
        "train_labeled.loc[m, \"from\"] = \"MMMLU\"\n",
        "train_labeled.loc[m, \"subject\"] = subj_long.fillna(subj_short)\n",
        "train_labeled = propagate_labels(train_labeled)\n",
        "\n",
        "print(\"MMMLU(paragraph) newly labeled:\", int(m.sum()))\n",
        "print(\"Total labeled so far:\", int(train_labeled[\"from\"].notna().sum()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2) 질문 텍스트 기반 보정\n",
        "remain = train_labeled[\"from\"].isna()\n",
        "train_q_norm = train_origin[\"question\"].map(lambda x: norm_text(x, remove_all_space=True)).astype(\"string\").fillna(\"\")\n",
        "train_labeled[\"question_norm\"] = train_q_norm\n",
        "\n",
        "q_head, q_mid, q_tail = build_anchor_views(train_q_norm, anchor_len=40)\n",
        "LONG_MIN, SHORT_MIN, SHORT_MAX = 20, 8, 29\n",
        "\n",
        "q_long_keys = make_keys(pd.concat([\n",
        "    q_head[remain & (train_q_norm.str.len() >= LONG_MIN)],\n",
        "    q_mid[remain & (train_q_norm.str.len() >= LONG_MIN)],\n",
        "    q_tail[remain & (train_q_norm.str.len() >= LONG_MIN)],\n",
        "], ignore_index=True).drop_duplicates(), min_len=LONG_MIN)\n",
        "q_short_keys = make_keys(train_q_norm[remain & train_q_norm.str.len().between(SHORT_MIN, SHORT_MAX)].drop_duplicates(), min_len=SHORT_MIN)\n",
        "\n",
        "top_q_long = build_top_subject_map(mmmlu_proc, q_long_keys, q_col=\"q_norm\", subj_col=\"Subject\", chunk_size=1000)\n",
        "top_q_short = build_top_subject_map(mmmlu_proc, q_short_keys, q_col=\"q_norm\", subj_col=\"Subject\", chunk_size=500)\n",
        "\n",
        "m_long = remain & (q_head.isin(top_q_long) | q_mid.isin(top_q_long) | q_tail.isin(top_q_long))\n",
        "m_short = remain & train_q_norm.isin(top_q_short)\n",
        "m = m_long | m_short\n",
        "\n",
        "subj_long = q_head.map(top_q_long).fillna(q_mid.map(top_q_long)).fillna(q_tail.map(top_q_long))\n",
        "subj_short = train_q_norm.map(top_q_short)\n",
        "\n",
        "train_labeled.loc[m, \"from\"] = \"MMMLU\"\n",
        "train_labeled.loc[m, \"subject\"] = subj_long.fillna(subj_short)\n",
        "train_labeled = propagate_labels(train_labeled)\n",
        "\n",
        "print(\"MMMLU(question) newly labeled:\", int(m.sum()))\n",
        "print(\"Total labeled so far:\", int(train_labeled[\"from\"].notna().sum()))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## KLUE-MRC 태깅\n",
        "- KLUE-MRC 전 split을 합친 뒤 허용 카테고리만 사용합니다.\n",
        "- 문단 앵커 기반 매핑으로 뉴스 카테고리를 subject로 설정합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ds_klue = load_dataset(\"klue\", \"mrc\")\n",
        "klue_raw = pd.concat([ds_klue[s].to_pandas() for s in ds_klue.keys()], ignore_index=True)\n",
        "\n",
        "ALLOWED = {\"경제\", \"교육산업\", \"국제\", \"부동산\", \"사회\", \"생활\", \"책마을\"}\n",
        "\n",
        "klue_proc = klue_raw.copy()\n",
        "klue_proc[\"news_category\"] = (\n",
        "    klue_proc[\"news_category\"]\n",
        "    .astype(\"string\")\n",
        "    .str.strip()\n",
        "    .replace({\"null\": pd.NA, \"NULL\": pd.NA, \"None\": pd.NA, \"\": pd.NA})\n",
        ")\n",
        "klue_proc = klue_proc[klue_proc[\"news_category\"].isin(ALLOWED)].copy()\n",
        "klue_proc[\"context_norm\"] = klue_proc[\"context\"].map(lambda x: norm_text(x, remove_all_space=True)).astype(\"string\").fillna(\"\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "remain = train_labeled[\"from\"].isna()\n",
        "s = train_origin[\"para_norm\"].astype(\"string\").fillna(\"\")\n",
        "a_head, a_mid, a_tail = build_anchor_views(s, anchor_len=40)\n",
        "\n",
        "LONG_MIN, SHORT_MIN, SHORT_MAX = 20, 8, 29\n",
        "long_keys = make_keys(pd.concat([\n",
        "    a_head[remain & (s.str.len() >= LONG_MIN)],\n",
        "    a_mid[remain & (s.str.len() >= LONG_MIN)],\n",
        "    a_tail[remain & (s.str.len() >= LONG_MIN)],\n",
        "], ignore_index=True).drop_duplicates(), min_len=LONG_MIN)\n",
        "short_keys = make_keys(s[remain & s.str.len().between(SHORT_MIN, SHORT_MAX)].drop_duplicates(), min_len=SHORT_MIN)\n",
        "\n",
        "top_long = build_top_subject_map(klue_proc, long_keys, q_col=\"context_norm\", subj_col=\"news_category\", chunk_size=1000)\n",
        "top_short = build_top_subject_map(klue_proc, short_keys, q_col=\"context_norm\", subj_col=\"news_category\", chunk_size=500)\n",
        "\n",
        "m_long = remain & (a_head.isin(top_long) | a_mid.isin(top_long) | a_tail.isin(top_long))\n",
        "m_short = remain & s.isin(top_short)\n",
        "m = m_long | m_short\n",
        "\n",
        "subj_long = a_head.map(top_long).fillna(a_mid.map(top_long)).fillna(a_tail.map(top_long))\n",
        "subj_short = s.map(top_short)\n",
        "\n",
        "train_labeled.loc[m, \"from\"] = \"klue-mrc\"\n",
        "train_labeled.loc[m, \"subject\"] = subj_long.fillna(subj_short)\n",
        "train_labeled = propagate_labels(train_labeled)\n",
        "\n",
        "print(\"KLUE-MRC newly labeled:\", int(m.sum()))\n",
        "print(\"Total labeled so far:\", int(train_labeled[\"from\"].notna().sum()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d84d725",
      "metadata": {},
      "outputs": [],
      "source": [
        "train_labeled['from'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 남은 데이터 확인 및 저장\n",
        "- 라벨이 비어 있는 샘플 수를 체크합니다.\n",
        "- 작업용 정규화 컬럼은 저장 전 제거해 원본 스키마를 유지합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "remaining = train_labeled[\"from\"].isna().sum()\n",
        "print(\"Unlabeled samples remaining:\", int(remaining))\n",
        "\n",
        "export_df = train_labeled.drop(columns=[c for c in [\"para_norm\", \"question_norm\", 'problems'] if c in train_labeled.columns])\n",
        "export_df.to_csv(OUTPUT_PATH, index=False)\n",
        "print(\"Saved:\", OUTPUT_PATH.resolve())\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
